{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"En clair.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[{"file_id":"1vLHK2Q9i7iXN4GLODhkjziSSFJUBqU-I","timestamp":1523286126921}],"collapsed_sections":["nAvD9AbQ7ifo","zwA2Q_5nObOY","0JQQGDIREqpA","48rcbcOHN0b-"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"nAvD9AbQ7ifo","colab_type":"text"},"cell_type":"markdown","source":["# IMPORTS"]},{"metadata":{"id":"Z7FX9YGt7T4a","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#INSTALLATIONS\n","!mkdir models\n","!pip install -q keras\n","import keras\n","!pip install -q tqdm\n","import tqdm\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","\n","#LE FUTUR WESH\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","#CLASSIQUES\n","from google.colab import files, auth\n","from oauth2client.client import GoogleCredentials\n","import numpy as np\n","np.random.seed(1000)\n","import os\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import timeit\n","import sklearn.metrics as sklm\n","import copy\n","import types as python_types\n","import warnings\n","import pickle\n","\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  print('ATTENTION ! PAS DE GPU !')\n","else:\n","  print('Found GPU at: {}'.format(device_name))\n","\n","#KERAS\n","import keras\n","from keras import activations, initializers, regularizers, constraints,metrics\n","from keras.legacy import interfaces\n","from keras.engine import InputSpec, Layer\n","from keras.layers import Input,BatchNormalization\n","from keras.layers.core import Reshape, Dense, Dropout, Flatten\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import Conv2D, UpSampling2D, MaxPooling2D\n","from keras.models import Model, Sequential\n","from keras.datasets import mnist\n","from keras.optimizers import Adam, Adagrad\n","from keras import backend as K\n","K.set_image_dim_ordering('tf')\n","from keras.utils import np_utils\n","from keras.utils.generic_utils import func_dump, func_load, deserialize_keras_object\n","\n","# DRIVE AUTHENTIFICATION\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zwA2Q_5nObOY","colab_type":"text"},"cell_type":"markdown","source":["# CONSTANTES"]},{"metadata":{"id":"KpMFPeUSKcP-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# QUELQUES VARIABLES\n","\n","adagrad = Adagrad(lr=0.001, epsilon=None, decay=0.0) #c'est celui-là qui est utilisé pour tous les modèles\n","adam = Adam(lr=0.0002, beta_1=0.5)\n","\n","randomVectorSize = 100\n","\n","d_losses = []\n","g_losses = []\n","c_losses = []\n","r_losses = []\n","\n","# Nombre de learning steps par batch du DISCRIMINATOR\n","nb_step_discriminator = 1\n","\n","# Seuil de score du DISCRIMINATOR en deça duquel on considere un exemple fake comme exemple classe 1 du REJECTOR\n","seuil_rejet_RejectionTraining = 0.45\n","\n","# Seuil de score du DISCRIMINATOR au-delà duquel on considere un exemple fake comme exemple classe 0 du REJECTOR\n","seuil_accept_RejectionTraining = 0.55\n","\n","# Nombre maximum d'exemples dans allGeneratedSamples\n","max_GeneratedSamples = 20000\n","\n","# Permutation aléatoire des exemples dans allGeneratedSamples lorsque max_GeneratedSamples exemples est atteint\n","randomPermut_GeneratedSamples = True\n","\n","# Tri des exemples dans allGeneratedSamples en fonction de leurs scores\n","sort_GeneratedSamples = False\n","\n","# Cible des exemples negatifs \n","target_neg = int(0)\n","\n","# Cible des exemples positifs \n","target_pos = int(1)\n","\n","#Seuil à appliquer pour les prédictions du REJECTOR\n","seuil_rejector = 0.5\n","\n","#Seuil à appliquer aux sorties du REJECTOR lors de l'entraînement du CLASSIFIER\n","seuil_rejector_to_train_classifier = 0.0\n","\n","# Taux de decroisance de la variance des rbf units\n","Decrease_rate_alpha = 1.1  \n","\n","# Utilisation des données perturbées\n","usePerturbatedData = False\n","\n","# Nombre de classes dans MNIST\n","nb_classes = 10\n","\n","# Choix des classes à traiter par le réseau\n","classes_train = np.random.choice(range(nb_classes), size=3, replace=False)\n","classes_train =[0, 4, 7]\n","classes_not_train = [i for i in range(nb_classes) if i not in classes_train]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3_yGMLAQClSn","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":71},"outputId":"a99629cb-663d-440c-fdba-a8bcb0c3b7c6","executionInfo":{"status":"ok","timestamp":1526548429503,"user_tz":-120,"elapsed":4984,"user":{"displayName":"Antoine Zyxado","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102674204708073293192"}}},"cell_type":"code","source":["# IMPORTATION MNIST\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","X_train = (X_train.astype(np.float32).reshape((X_train.shape[0],28,28,1)) - 127.5)/127.5\n","Y_train = np_utils.to_categorical(y_train, nb_classes)\n","#Y_train = Y_train * 0.9\n","X_test = (X_test.astype(np.float32).reshape((X_test.shape[0],28,28,1)) - 127.5)/127.5\n","Y_test = np_utils.to_categorical(y_test, nb_classes)\n","#Y_test = Y_test * 0.9\n","\n","indices = np.where(np.isin(y_train, classes_train))\n","X_train_restricted = X_train[indices,:]\n","X_train_restricted = X_train_restricted.reshape(X_train_restricted.shape[1],28,28,1)\n","Y_train_restricted = Y_train[indices,:]\n","Y_train_restricted = Y_train_restricted.reshape(Y_train_restricted.shape[1],10)\n","\n","indices = np.where(np.isin(y_train, classes_not_train))\n","X_train_out = X_train[indices,:]\n","X_train_out = X_train_out.reshape(X_train_out.shape[1],28,28,1)\n","Y_train_out = Y_train[indices,:]\n","Y_train_out = Y_train_out.reshape(Y_train_out.shape[1],10)\n","\n","indices = np.where(np.isin(y_test, classes_train))\n","X_test_restricted = X_test[indices,:]\n","X_test_restricted = X_test_restricted.reshape(X_test_restricted.shape[1],28,28,1)\n","Y_test_restricted = Y_test[indices,:]\n","Y_test_restricted = Y_test_restricted.reshape(Y_test_restricted.shape[1],10)\n","\n","indices = np.where(np.isin(y_test, classes_not_train))\n","X_test_out = X_test[indices,:]\n","X_test_out = X_test_out.reshape(X_test_out.shape[1],28,28,1)\n","Y_test_out = Y_test[indices,:]\n","Y_test_out = Y_test_out.reshape(Y_test_out.shape[1],10)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 3s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"CqhQt-zg9CX-","colab_type":"text"},"cell_type":"markdown","source":["# FONCTION UTILITAIRES"]},{"metadata":{"id":"YnkhTU9v9Jkf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"form"},"cell_type":"code","source":["#@title CALCUL DE PERFORMANCES\n","###________FONCTIONS POUR CALCULER LES PERFORMANCES________###\n","\n","#MOYENNE DES PREDICTIONS CORRECTES\n","def mean_pred_pos(y_true, y_pred):\n","  good_examples = np.where(y_true[:,0] ==0)\n","  return K.mean(y_pred[good_examples])\n","  \n","#MOYENNE DES PREDICTIONS FAUSSES\n","def mean_pred_neg(y_true, y_pred):\n","  good_examples = np.where(y_true[:,0] ==1)\n","  return K.mean(y_pred)\n","\n","#RECALL RATE (vrais positifs / (vrais positifs + faux négatifs))\n","def recall(y_true, y_pred):\n","  res  = K.eval(tf.multiply (y_true, y_pred))\n","  true_positives = np.sum(np.round(np.clip(res, 0, 1)))\n","  possible_positives = np.sum(np.round(np.clip(y_true, 0, 1)))\n","  recall = true_positives / (possible_positives + K.epsilon())\n","  return recall\n","\n","#PRECISION (vrais positifs / (vrais positifs + faux positifs))\n","def precision(y_true, y_pred):\n","  res  = K.eval(tf.multiply (y_true, y_pred))\n","  true_positives = np.sum(np.round(np.clip(res, 0, 1)))\n","  predicted_positives = np.sum(np.round(np.clip(y_pred, 0, 1)))\n","  precision = true_positives / (predicted_positives + K.epsilon())\n","  return precision\n","\n","#F1 = moyenne harmonique du recall rate et de la précision\n","def f1(y_true, y_pred):\n","    precision_f1 = precision(y_true, y_pred)\n","    recall_f1 = recall(y_true, y_pred)\n","    return 2*((precision_f1*recall_f1)/(precision_f1+recall_f1))\n","\n","#AREA UNDER CURVE de la courbe Faux positifs/Vrais positifs...\n","def auc(y_true, y_pred):  \n","  fpr, tpr, thresholds = sklm.roc_curve(y_true, y_pred, pos_label=1)\n","  return sklm.auc(fpr, tpr)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nLgrEQe8DXFQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"form"},"cell_type":"code","source":["#@title COUCHES RBF\n","###________TROIS CLASSES POUR IMPLEMENTER DES COUCHES RBF________###\n","\n","class RBFLayer(Layer):\n","    def __init__(self, alpha=1000.0, alpha_initializer='zeros',\n","                 alpha_regularizer=None,\n","                 alpha_constraint=None, \n","                 **kwargs):\n","        super(RBFLayer, self).__init__(**kwargs)\n","        self.supports_masking = True\n","        self.supports_masking = True\n","        self.alpha_initializer = keras.initializers.Constant(value=1000.0)\n","        self.alpha_regularizer = regularizers.get(alpha_regularizer)\n","        self.alpha_constraint = constraints.get(alpha_constraint)\n","        self.alpha = self.add_weight(shape=(1,),\n","                                     name='alpha',\n","                                     initializer=self.alpha_initializer,\n","                                     regularizer=self.alpha_regularizer,\n","                                     constraint=self.alpha_constraint)\n","        self.trainable = False\n","\n","    def call(self, inputs):\n","        y = - inputs / self.alpha[0]\n","        y = keras.backend.exp(y)\n","        return y\n","\n","    def get_config(self):\n","        config = {\n","            'alpha_initializer': initializers.serialize(self.alpha_initializer),\n","            'alpha_regularizer': regularizers.serialize(self.alpha_regularizer),\n","            'alpha_constraint': constraints.serialize(self.alpha_constraint),\n","            'shared_axes': self.shared_axes\n","        }\n","        base_config = super(RBFLayer, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","class RBF(Layer):\n","    @interfaces.legacy_dense_support\n","    def __init__(self, units,\n","                 activation=None,\n","                 use_bias=False,\n","                 kernel_initializer='TruncatedNormal',\n","                 bias_initializer='zeros',\n","                 kernel_regularizer=None,\n","                 bias_regularizer=None,\n","                 activity_regularizer=None,\n","                 kernel_constraint=None,\n","                 bias_constraint=None,\n","                 sigma = 1000.0,\n","                 **kwargs):\n","        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n","            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n","        super(RBF, self).__init__(**kwargs)\n","        self.units = units\n","        self.activation = activations.get(activation)\n","        self.use_bias = use_bias\n","        self.kernel_initializer = initializers.get(kernel_initializer)\n","        self.bias_initializer = initializers.get(bias_initializer)\n","        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n","        self.bias_regularizer = regularizers.get(bias_regularizer)\n","        self.activity_regularizer = regularizers.get(activity_regularizer)\n","        self.kernel_constraint = constraints.get(kernel_constraint)\n","        self.bias_constraint = constraints.get(bias_constraint)\n","        self.sigma = sigma\n","        self.input_spec = InputSpec(min_ndim=2)\n","        self.supports_masking = True\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) >= 2\n","        input_dim = input_shape[-1]\n","\n","        self.kernel = self.add_weight(shape=(input_dim, self.units),\n","                                      initializer=self.kernel_initializer,\n","                                      name='kernel',\n","                                      regularizer=self.kernel_regularizer,\n","                                      constraint=self.kernel_constraint)\n","        if self.use_bias:\n","            self.bias = self.add_weight(shape=(self.units,),\n","                                        initializer=self.bias_initializer,\n","                                        name='bias',\n","                                        regularizer=self.bias_regularizer,\n","                                        constraint=self.bias_constraint)\n","        else:\n","            self.bias = None\n","        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n","        self.built = True\n","\n","    def call(self, inputs):\n","        norm_x = K.transpose(K.sum(inputs * inputs , axis=1))\n","        norm_w = K.sum(self.kernel *self.kernel , axis=0)\n","        norm_x = keras.backend.expand_dims(norm_x, axis=-1)\n","        prod_scal = -2 * K.dot(inputs, self.kernel)\n","        print (\"Shapes : \", norm_x.shape, norm_w.shape, prod_scal.shape) \n","        a = tf.add(norm_x, prod_scal) # -2 *  prod_scal)\n","        y = tf.add(a, norm_w )\n","        y = - y / self.sigma\n","        y = keras.backend.exp(y)\n","        return y\n","\n","    def compute_output_shape(self, input_shape):\n","        assert input_shape and len(input_shape) >= 2\n","        assert input_shape[-1]\n","        output_shape = list(input_shape)\n","        output_shape[-1] = self.units\n","        return tuple(output_shape)\n","\n","    def get_config(self):\n","        config = {\n","            'units': self.units,\n","            'activation': activations.serialize(self.activation),\n","            'use_bias': self.use_bias,\n","            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n","            'bias_initializer': initializers.serialize(self.bias_initializer),\n","            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n","            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n","            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n","            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n","            'bias_constraint': constraints.serialize(self.bias_constraint)\n","        }\n","        base_config = super(RBF, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","class RBF2(Layer):\n","    @interfaces.legacy_dense_support\n","    def __init__(self, units,\n","                 activation=None,\n","                 use_bias=False,\n","                 kernel_initializer='TruncatedNormal',\n","                 bias_initializer='zeros',\n","                 kernel_regularizer=None,\n","                 bias_regularizer=None,\n","                 activity_regularizer=None,\n","                 kernel_constraint=None,\n","                 bias_constraint=None,\n","                 **kwargs):\n","        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n","            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n","        super(RBF2, self).__init__(**kwargs)\n","        self.units = units\n","        self.activation = activations.get(activation)\n","        self.use_bias = use_bias\n","        self.kernel_initializer = initializers.get(kernel_initializer)\n","        self.bias_initializer = initializers.get(bias_initializer)\n","        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n","        self.bias_regularizer = regularizers.get(bias_regularizer)\n","        self.activity_regularizer = regularizers.get(activity_regularizer)\n","        self.kernel_constraint = constraints.get(kernel_constraint)\n","        self.bias_constraint = constraints.get(bias_constraint)\n","        self.input_spec = InputSpec(min_ndim=2)\n","        self.supports_masking = True\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) >= 2\n","        input_dim = input_shape[-1]\n","        self.kernel = self.add_weight(shape=(input_dim, self.units),\n","                                      initializer=self.kernel_initializer,\n","                                      name='kernel',\n","                                      regularizer=self.kernel_regularizer,\n","                                      constraint=self.kernel_constraint)\n","        if self.use_bias:\n","            self.bias = self.add_weight(shape=(self.units,),\n","                                        initializer=self.bias_initializer,\n","                                        name='bias',\n","                                        regularizer=self.bias_regularizer,\n","                                        constraint=self.bias_constraint)\n","        else:\n","            self.bias = None\n","        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n","        self.built = True\n","\n","    def call(self, inputs):\n","        norm_x = K.transpose(K.sum(inputs * inputs , axis=1))\n","        norm_w = K.sum(self.kernel *self.kernel , axis=0)\n","        norm_x = keras.backend.expand_dims(norm_x, axis=-1)\n","        prod_scal = -2 * K.dot(inputs, self.kernel)\n","        print (\"Shapes : \", norm_x.shape, norm_w.shape, prod_scal.shape) \n","        a = tf.add(norm_x, prod_scal) # -2 *  prod_scal)\n","        y = tf.add(a, norm_w )\n","        return y\n","\n","    def compute_output_shape(self, input_shape):\n","        assert input_shape and len(input_shape) >= 2\n","        assert input_shape[-1]\n","        output_shape = list(input_shape)\n","        output_shape[-1] = self.units\n","        return tuple(output_shape)\n","\n","    def get_config(self):\n","        config = {\n","            'units': self.units,\n","            'activation': activations.serialize(self.activation),\n","            'use_bias': self.use_bias,\n","            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n","            'bias_initializer': initializers.serialize(self.bias_initializer),\n","            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n","            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n","            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n","            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n","            'bias_constraint': constraints.serialize(self.bias_constraint)\n","        }\n","        base_config = super(RBF2, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t0_aQmwuQvdc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"form"},"cell_type":"code","source":["#@title AFFICHAGE\n","###________FONCTIONS D'AFFICHAGE________###\n","\n","# AFFICHAGE DES LOSS DU GAN\n","def plotLoss(epoch):\n","    plt.figure(figsize=(10, 8))\n","    plt.plot(d_losses, label='Discriminitive loss')\n","    plt.plot(g_losses, label='Generative loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.savefig('loss_e%d.png' % epoch)\n","    uploaded = drive.CreateFile({'title': 'loss_e%d.png' % epoch})\n","    uploaded.SetContentFile('loss_e%d.png' % epoch)\n","    uploaded.Upload()\n","\n","# EXEMPLES D'IMAGES GENEREES\n","def plotGeneratedImages(epoch, examples=100):\n","    noise = np.random.normal(0, 1, size=[examples, randomVectorSize])\n","    generatedImages = generator.predict(noise)\n","    generatedImages = generatedImages.reshape(examples, 28, 28)\n","    Xg = generatedImages[:8]\n","    nom_file = \"gan_generated_image_epoch_\" +str(epoch) + \".png\"\n","    print (nom_file)\n","    np.save(nom_file, Xg)\n","    #files.download(nom_file)\n","\n","# EXEMPLES D'IMAGES GENEREES 2\n","def plotGeneratedImages2(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n","    noise = np.random.normal(0, 1, size=[examples, randomVectorSize])\n","    generatedImages = generator.predict(noise)\n","    plt.figure(figsize=figsize)\n","    for i in range(generatedImages.shape[0]):\n","        plt.subplot(dim[0], dim[1], i+1)\n","        plt.imshow(generatedImages[i, 0], interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.savefig('gan_generated_image_epoch_%d.png' % epoch)\n","    files.download('gan_generated_image_epoch_%d.png' % epoch)    \n","\n","# AFFICHE LES IMAGES (mettez-en 100 ou moins svp sinon je sais pas ce que ça va faire)\n","def plotImages(images, dim=(10, 10), figsize=(10, 10)):\n","    plt.figure(figsize=figsize)\n","    for i in range(images.shape[0]):\n","        plt.subplot(dim[0], dim[1], i+1)\n","        plt.imshow(images[i, 0], interpolation='nearest', cmap='gray_r')\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.savefig('gan_perturbated_images.png')\n","    files.download('gan_perturbated_images.png')  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"7MatjXR7aOWd","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["#@title MISE EN FORME DE DONNEES\n","###________FONCTIONS DE MISE EN FORME DES DONNEES________###\n","\n","# MET EN FORME LES LABELS ET LES DONNEES POUR LE TEST DU CLASSIFIEUR SUR LES DONNEES NON REJETEES\n","def data_evaluation_classifier(X_real, y_real):\n","  y_dgan = rejector.predict(X_real)\n","  good_examples = np.where(y_dgan[:,0] < seuil_rejector)\n","  X_disc = X_real[good_examples]\n","  y_disc = y_real[good_examples]\n","  weights = np.ones((y_real.shape[0],1))\n","  return X_disc, y_disc\n","\n","# MET EN FORME LES LABELS POUR DES ENTREES X TOUTES VRAIES\n","def data_evaluation_rejector_true(X):\n","    return X, np.ones((X.shape[0],1))\n","\n","# MET EN FORME LES LABELS ET LES DONNEES POUR LES IMAGES GENEREES\n","def data_evaluation_rejector_fake(X_fake, filtre):\n","  if filtre:\n","    y_reject = discriminator.predict(X_fake)\n","    good_examples = np.where(y_reject[:,0] < seuil_rejet_RejectionTraining)\n","    X_disc = X_fake[good_examples]\n","    y_disc = np.zeros((X_disc.shape[0],1))\n","  else:\n","    X_disc = X_fake[:]\n","    y_disc = np.zeros((X_disc.shape[0],1))\n","  return X_disc, y_disc\n","\n","# MODIFIE DES IMAGES EN FAISANT DU COPIER/COLLER (à reprendre en modifiant)\n","def imageBatch_mix(batchsize):\n","  lesindices =  np.random.randint(0, X_train_restricted.shape[0], size=batchsize)\n","  imageBatch_to_perturb = X_train_restricted[lesindices,:]\n","  imageBatch_to_perturb = imageBatch_to_perturb.reshape((imageBatch_to_perturb.shape[0], X_train.shape[1]*X_train.shape[2]))\n","  data_dim = imageBatch_to_perturb.shape[1]\n","  X_perturbated = imageBatch_to_perturb\n","  rate1 = 0.9\n","  seuil1 = int(rate1* (float(data_dim)))\n","  seuil1 = np.random.randint(0, high=seuil1)\n","  X_perturbated[:,:seuil1] = imageBatch_to_perturb[:,:seuil1]\n","  rate2 = 0.9\n","  seuil2 = int(rate2* (float(data_dim)))\n","  seuil2 = np.random.randint(0, high=seuil2)\n","  seuil2 = min(seuil1+seuil2, data_dim)\n","  p = np.random.permutation(batchsize)\n","  X_perturbated[:,seuil1:seuil2] = imageBatch_to_perturb[p,seuil1:seuil2]\n","  p = np.random.permutation(batchsize)  \n","  X_perturbated[:,seuil2:] = imageBatch_to_perturb[p,seuil2:]\n","  X_perturbated = X_perturbated.reshape((imageBatch_to_perturb.shape[0], 28, 28, 1))\n","  return X_perturbated\n","\n","# RENVOIE UN BATCH D'ENTRAINEMENT POUR LE REJECTOR\n","def data_train_rejector(X_real, y_real, X_fake, X_perturbated):\n","    batchSize = X_real.shape[0]\n","    y_fake_reject = discriminator.predict(X_fake)\n","    good_examples = np.where(y_fake_reject[:,0] < seuil_rejet_RejectionTraining )\n","    bad_examples = np.where(y_fake_reject[:,0] > seuil_accept_RejectionTraining )\n","    X_fake_good = X_fake[good_examples[0]]\n","    X_fake_good= X_fake_good[:min(X_fake_good.shape[0],batchSize)]\n","    y_fake_good = np.zeros((y_fake_reject[good_examples[0]].shape[0],1))\n","    y_fake_good= y_fake_good[:min(X_fake_good.shape[0],batchSize)]\n","    y_fake_good[:,] = 0\n","    X_fake_bad = X_fake[bad_examples[0]]\n","    X_fake_bad = X_fake_bad[:min(X_fake_bad.shape[0],batchSize)]\n","    y_fake_bad = np.zeros((y_fake_reject[bad_examples[0]].shape[0],1))\n","    y_fake_bad = y_fake_bad[:min(X_fake_bad.shape[0],batchSize)]\n","    y_fake_bad[:,] = 1\n","    y_real_reject = np.ones((X_real.shape[0],1))\n","    y_perturbated = np.zeros((X_perturbated.shape[0],1))\n","    if usePerturbatedData:\n","      X = np.concatenate([X_real, X_fake_good, X_fake_bad, X_perturbated])\n","      y_rejection = np.concatenate([y_real_reject, y_fake_good, y_fake_bad, y_perturbated])\n","    else:\n","      X = np.concatenate([X_real, X_fake_good, X_fake_bad])   \n","      y_rejection = np.concatenate([y_real_reject, y_fake_good, y_fake_bad])\n","    weights = np.ones((y_rejection.shape[0],1))\n","    return X, y_rejection, weights\n","\n","# RENVOIE LES ELEMENTS DE X_REAL NON REJETES PAR LE REJECTOR (à modifier pour inclure les données perturbées)\n","def data_train_classifier(X_real, y_real, X_perturbated):\n","    y_dgan = rejector.predict(X_real)\n","    good_examples = np.where(y_dgan[:,0] > seuil_rejector_to_train_classifier)\n","    X_disc = X_real[good_examples]\n","    y_disc = y_real[good_examples]\n","    weights = np.ones((y_disc.shape[0],1))\n","    \"\"\"\n","    y_perturbated = np.zeros((X_perturbated.shape[0],y_disc.shape[1]))\n","    X = np.concatenate([X_disc, X_perturbated])\n","    y = np.concatenate([y_disc, y_perturbated])\n","    weights = np.ones((y.shape[0],1))\n","    \"\"\"\n","    return X_disc, y_disc, weights\n","\n","#APPLIQUE UNE PERMUTATION DES IMAGES GENEREES (optionnel) PUIS FILTRE LES IMAGES EN FONCTION DE LEUR REPONSE AU DISCRIMINATEUR\n","def filtrage(allgeneratedImages, nb_generated_max = max_GeneratedSamples):\n","    if randomPermut_GeneratedSamples:\n","      p = np.random.permutation(range(allgeneratedImages.shape[0]))\n","      allgeneratedImages = allgeneratedImages[p]\n","    y_fake_reject = discriminator.predict(allgeneratedImages)\n","    good_examples = np.where(y_fake_reject[:,0] < seuil_rejet_RejectionTraining )\n","    X_temp = allgeneratedImages[good_examples]\n","    if (X_temp.shape[0]> nb_generated_max):\n","        return X_temp[:nb_generated_max]\n","    else:\n","        if (X_temp.shape[0]==0):\n","            return allgeneratedImages[:min(allgeneratedImages.shape[0],nb_generated_max)]\n","        else:\n","            return X_temp          \n","          \n","# SAUVEGARDER LES MODELES\n","def saveModels(epoch):\n","  generator.save('generator_e%d' % epoch)\n","  discriminator.save('discriminator_e%d' % epoch)\n","\n","# TELECHARGER LES MODELES\n","def downloadModels(epoch):\n","  uploaded = drive.CreateFile({'title': 'generator_e%d' % epoch})\n","  uploaded.SetContentFile('generator_e%d' % epoch)\n","  uploaded.Upload()\n","  uploaded = drive.CreateFile({'title': 'discriminator_e%d' % epoch})\n","  uploaded.SetContentFile('discriminator_e%d' % epoch)\n","  uploaded.Upload()\n","\n","# UPLOADER LES MODELES\n","def uploadModels(epoch):\n","  genID = drive.ListFile({'title':'generator_e%d' % epoch}).GetList()[0]['id']\n","  gen = drive.CreateFile({'id':genID})\n","  gen.GetContentFile('generator_e%d' % epoch)\n","  generator = keras.load_model('generator_e%d' % epoch)\n","  discID = drive.ListFile({'title':'discriminator_e%d' % epoch}).GetList()[0]['id']\n","  disc = drive.CreateFile({'id':discID})\n","  disc.GetContentFile('discriminator_e%d' % epoch)\n","  discriminator = keras.load_model('discriminator_e%d' % epoch)\n","  return generator, discriminator\n","\n","#Je sais pas encore ni ce que c'est ni à quoi ça sert, ça ne semble pas utilisé\n","def toutes_data_evaluation_classifier(X, y):\n","  y_disc = np.concatenate((np.zeros((y.shape[0],1)), y ), axis=1)\n","  return X, y_disc\n","\n","# Ne semble pas utilisé\n","def imageBatch_neg(batchsize):\n","  lesindices =  np.random.randint(0, X_train.shape[0], size=batchsize*3)\n","  imageBatch_to_perturb = X_train[lesindices,:]\n","  X_perturbated = imageBatch_to_perturb[:batchsize] + imageBatch_to_perturb[batchsize:2*batchsize]- imageBatch_to_perturb[2*batchsize:3*batchsize]\n","  #X_perturbated = (imageBatch_to_perturb[:batchsize] + imageBatch_to_perturb[batchsize:2*batchsize])/2\n","  return X_perturbated"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lESUXxCjMF4u","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"form"},"cell_type":"code","source":["#@title SEUILLAGES\n","\n","#Application d'un seuil en sortie du classifier sur la probabilité la plus grande\n","def test_model(model,seuil,x_test,y_test):\n","  print(\"Seuillage sur la probabilité max :\")\n","  print(\"Seuil = \"+str(seuil))\n","  predictions=model.predict(x_test)\n","  indices=[]\n","  for i in range(len(x_test)):\n","    #print(max(predictions[i]))\n","    if max(predictions[i])>seuil:\n","      indices.append(i)\n","  new_x_test=[]\n","  new_y_test=[]\n","  if len(indices)==0:\n","    print(\"Toutes les images sont rejetées...\")\n","    return([len(x_test),1])\n","  for indice in indices:\n","    new_x_test.append(x_test[indice])\n","    new_y_test.append(y_test[indice])\n","  new_x_test=np.array(new_x_test)\n","  new_y_test=np.array(new_y_test)\n","  new_score=model.evaluate(new_x_test,new_y_test,batch_size=16)\n","  nb_total=len(x_test)\n","  nb_conservees=len(new_x_test)\n","  nb_rejetees=nb_total-nb_conservees\n","  rapport_conservees=nb_conservees/nb_total\n","  print(\"Nombre d'images conservées : \"+str(nb_conservees)+\"/\"+str(nb_total)+\" = \"+str(100*rapport_conservees)+\"%\")\n","  print(\"Nombre d'images rejetées : \"+str(nb_rejetees))\n","  print(\"Score avec les images conservées : \",new_score)\n","  return([nb_rejetees,new_score[1]])\n","\n","\n","#Calcul de l'entropie\n","def entropie(array):\n","  H=0\n","  for j in range(len(array)):\n","    H+=-1*array[j]*np.log2(array[j])\n","  return(H)\n","\n","\n","#Application d'un seuil sur l'entropie de la sortie du classifier\n","def test_model_entropie(model,seuil,x_test,y_test):\n","  print(\"Seuillage sur l'entropie : \")\n","  print(\"Seuil = \"+str(seuil))\n","  predictions=model.predict(x_test)\n","  indices=[]\n","  for i in range(len(x_test)):\n","    #print(entropie(predictions[i]))\n","    if entropie(predictions[i])<seuil:\n","      indices.append(i)\n","  new_x_test=[]\n","  new_y_test=[]\n","  if len(indices)==0:\n","    print(\"Toutes les images sont rejetées...\")\n","    return([len(x_test),1])\n","  for indice in indices:   \n","    new_x_test.append(x_test[indice])\n","    new_y_test.append(y_test[indice])\n","  new_x_test=np.array(new_x_test) \n","  new_y_test=np.array(new_y_test)\n","  new_score=model.evaluate(new_x_test,new_y_test,batch_size=16)\n","  nb_total=len(x_test)\n","  nb_conservees=len(new_x_test)\n","  nb_rejetees=nb_total-nb_conservees\n","  rapport_conservees=nb_conservees/nb_total\n","  print(\"Nombre d'images conservées : \"+str(nb_conservees)+\"/\"+str(nb_total)+\" = \"+str(100*rapport_conservees)+\"%\")\n","  print(\"Nombre d'images rejetées : \"+str(nb_rejetees))\n","  print(\"Score avec les images conservées : \",new_score)\n","  return([nb_rejetees,new_score[1]])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0JQQGDIREqpA","colab_type":"text"},"cell_type":"markdown","source":["# MODELES"]},{"metadata":{"id":"fN5GI4c1KyOR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":476},"outputId":"4d133515-96ca-4639-d7cd-35107d946b1b","executionInfo":{"status":"ok","timestamp":1526548433456,"user_tz":-120,"elapsed":408,"user":{"displayName":"Antoine Zyxado","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102674204708073293192"}}},"cell_type":"code","source":["# Generator\n","\n","g_input = Input(shape=(randomVectorSize,),name=\"g_input\")\n","x = Dense(7*7*128, kernel_initializer=initializers.RandomNormal(stddev=0.02))  (g_input)\n","x = LeakyReLU(0.2)                                                             (x)\n","x = Reshape((7,7,128))                                                         (x)\n","x = UpSampling2D(size=(2, 2))                                                  (x)\n","x = Conv2D(64, kernel_size=(5, 5), padding='same')                             (x)\n","x = LeakyReLU(0.2)                                                             (x)\n","x = UpSampling2D(size=(2, 2))                                                  (x)\n","g_prediction = Conv2D(1, kernel_size=(5, 5), padding='same', activation='tanh')(x)\n","\n","generator = Model(input = g_input, output = g_prediction)\n","generator.compile(optimizer=adagrad, loss='binary_crossentropy')\n","generator.summary()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","g_input (InputLayer)         (None, 100)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 6272)              633472    \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 6272)              0         \n","_________________________________________________________________\n","reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 14, 14, 64)        204864    \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","up_sampling2d_2 (UpSampling2 (None, 28, 28, 64)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 28, 28, 1)         1601      \n","=================================================================\n","Total params: 839,937\n","Trainable params: 839,937\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"g_..., outputs=Tensor(\"co...)`\n","  if sys.path[0] == '':\n"],"name":"stderr"}]},{"metadata":{"id":"bT5meen2EsxQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":476},"outputId":"3790c2f6-9993-4bf5-b8c5-e44e39176a59","executionInfo":{"status":"ok","timestamp":1526548434027,"user_tz":-120,"elapsed":416,"user":{"displayName":"Antoine Zyxado","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102674204708073293192"}}},"cell_type":"code","source":["# Discriminator\n","\n","d_input = Input(shape=(28,28,1),name=\"d_input\")\n","x = Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', kernel_initializer=initializers.RandomNormal(stddev=0.02)) (d_input)\n","x = LeakyReLU(0.2)                                                 (x)\n","x = Dropout(0.3)                                                   (x)\n","x = Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same')(x)\n","x = LeakyReLU(0.2)                                                 (x)\n","x = Dropout(0.3)                                                   (x)\n","x = Flatten()                                                      (x)\n","d_prediction = Dense(1, activation='sigmoid', name='d_output')     (x)\n","\n","discriminator = Model(input = d_input, output = d_prediction)\n","discriminator.compile(optimizer=adagrad, loss='binary_crossentropy')\n","discriminator.summary()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","d_input (InputLayer)         (None, 28, 28, 1)         0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 14, 14, 64)        1664      \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 7, 7, 128)         204928    \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 6272)              0         \n","_________________________________________________________________\n","d_output (Dense)             (None, 1)                 6273      \n","=================================================================\n","Total params: 212,865\n","Trainable params: 212,865\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"d_..., outputs=Tensor(\"d_...)`\n","  if sys.path[0] == '':\n"],"name":"stderr"}]},{"metadata":{"id":"uce7_osINqgj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":272},"outputId":"e55c9f47-5c80-40b9-bea1-127fe3eb136d","executionInfo":{"status":"ok","timestamp":1526548434567,"user_tz":-120,"elapsed":395,"user":{"displayName":"Antoine Zyxado","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102674204708073293192"}}},"cell_type":"code","source":["# GAN\n","\n","discriminator.trainable = False\n","gan_input = Input(shape=(randomVectorSize,))\n","x = generator (gan_input)\n","gan_prediction = discriminator (x)\n","\n","gan = Model(input = gan_input, output = gan_prediction)\n","gan.compile(optimizer=adagrad, loss='binary_crossentropy')\n","gan.summary()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 100)               0         \n","_________________________________________________________________\n","model_1 (Model)              (None, 28, 28, 1)         839937    \n","_________________________________________________________________\n","model_2 (Model)              (None, 1)                 212865    \n","=================================================================\n","Total params: 1,052,802\n","Trainable params: 839,937\n","Non-trainable params: 212,865\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"mo...)`\n","  import sys\n"],"name":"stderr"}]},{"metadata":{"id":"L1b4xOugDv8V","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":476},"outputId":"fcde2108-a393-49fa-ecd7-67e85aa8adf5","executionInfo":{"status":"ok","timestamp":1526548435131,"user_tz":-120,"elapsed":518,"user":{"displayName":"Antoine Zyxado","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102674204708073293192"}}},"cell_type":"code","source":["# Classifier\n","\n","c_input = Input(shape=(28,28,1), name='c_input')\n","x =  Conv2D(64, kernel_size=(3, 3), activation='relu') (c_input)\n","x = MaxPooling2D(pool_size=(2, 2))                     (x)\n","x = Conv2D(128, (3, 3), activation='relu')             (x)\n","x = MaxPooling2D(pool_size=(2, 2))                     (x)\n","x = Dropout(0.25)                                      (x)\n","x = Flatten()                                          (x)\n","x = Dense(128, activation='relu')                      (x)\n","#x = Dropout(0.5)                                       (x)\n","c_prediction = Dense(10, activation='softmax', name='c_output') (x)\n","\n","classifier = Model(input = c_input, output = c_prediction)\n","classifier.compile(optimizer=adagrad, loss= 'binary_crossentropy', metrics=[metrics.categorical_accuracy])\n","classifier.summary()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","c_input (InputLayer)         (None, 28, 28, 1)         0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 26, 26, 64)        640       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 13, 13, 64)        0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 11, 11, 128)       73856     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 5, 5, 128)         0         \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 5, 5, 128)         0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 3200)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 128)               409728    \n","_________________________________________________________________\n","c_output (Dense)             (None, 10)                1290      \n","=================================================================\n","Total params: 485,514\n","Trainable params: 485,514\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"c_..., outputs=Tensor(\"c_...)`\n","  del sys.path[0]\n"],"name":"stderr"}]},{"metadata":{"id":"xieokWfyHqji","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":544},"outputId":"eedac7e8-bc11-49be-f1b0-206670d7dae9","executionInfo":{"status":"ok","timestamp":1526548435638,"user_tz":-120,"elapsed":400,"user":{"displayName":"Antoine Zyxado","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102674204708073293192"}}},"cell_type":"code","source":["# Rejector\n","\n","r_input = Input(shape=(28,28,1), name='r_input')\n","x =  Conv2D(64, kernel_size=(3, 3), activation='relu')(r_input)\n","x = MaxPooling2D(pool_size=(3, 3))                    (x)\n","x = Conv2D(128, (3, 3), activation='relu')            (x)\n","x = MaxPooling2D(pool_size=(3, 3))                    (x)\n","x = Dropout(0.25)                                     (x)\n","x = Flatten()                                         (x)\n","#x = RBF(128, sigma=1000.0)                            (x)\n","x = Dense(128, activation='relu')                     (x)\n","x = RBF2(300)                                         (x)\n","x = RBFLayer(alpha=2000.0)                            (x)\n","#x = BatchNormalization()                              (x)\n","#x = Dropout(0.5)                                      (x)\n","r_prediction = Dense(1, activation='tanh', name='r_output', use_bias=False) (x)\n","\n","rejector = Model(input = r_input, output = r_prediction)\n","rejector.compile(optimizer=adagrad, loss= 'mse', metrics=['acc'])  #  metrics=['acc', f1, precision, recall])\n","rejector.summary()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Shapes :  (?, 1) (300,) (?, 300)\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","r_input (InputLayer)         (None, 28, 28, 1)         0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 26, 26, 64)        640       \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 6, 6, 128)         73856     \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 2, 2, 128)         0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 2, 2, 128)         0         \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 512)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 128)               65664     \n","_________________________________________________________________\n","rb_f2_1 (RBF2)               (None, 300)               38400     \n","_________________________________________________________________\n","rbf_layer_1 (RBFLayer)       (None, 300)               1         \n","_________________________________________________________________\n","r_output (Dense)             (None, 1)                 300       \n","=================================================================\n","Total params: 178,861\n","Trainable params: 178,860\n","Non-trainable params: 1\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"r_..., outputs=Tensor(\"r_...)`\n"],"name":"stderr"}]},{"metadata":{"id":"VjfXQ5zcVWvU","colab_type":"text"},"cell_type":"markdown","source":["# TRAININGS ET BASES DE DONNEES"]},{"metadata":{"id":"5EBjYfnOVZSb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"form"},"cell_type":"code","source":["#@title ENTRAINEMENT DE TOUS LES RESEAUX\n","def train_all(X_train, Y_train, X_test_in, Y_test_in, X_test_out, Y_test_out, epochs=100, batchSize=128):\n","  #INITIALISATION...\n","  batchCount = int(X_train.shape[0] / batchSize)\n","  print ('Epochs:', epochs)\n","  print ('Batch size:', batchSize)\n","  print ('Batches per epoch:', batchCount)\n","  noise = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","  allgeneratedImages = generator.predict(noise)\n","\n","  for e in range(1, epochs+1):\n","    print ('-'*15, 'Epoch %d' % e, '-'*15)\n","    rloss=0\n","    closs=0\n","    dloss=0\n","    gloss=0\n","    \n","    # ENTRAINEMENT RESEAUX\n","    for _ in tqdm(range(batchCount)):\n","      # DISCRIMINATOR\n","      noise = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      generatedImages = generator.predict(noise)\n","      imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n","      X = np.concatenate([imageBatch, generatedImages])\n","      yDis = np.zeros(2*batchSize)\n","      yDis[:batchSize] = 0.9\n","      dloss += discriminator.train_on_batch(X, yDis)\n","\n","      # GENERATOR\n","      noise = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      yGen = np.ones(batchSize)\n","      discriminator.trainable = False\n","      gloss += gan.train_on_batch(noise, yGen)\n","      \n","      # RECUPERATION DES IMAGES GENEREES PERTINENTES\n","      allgeneratedImages = np.concatenate ([allgeneratedImages, generatedImages])\n","      allgeneratedImages = filtrage(allgeneratedImages)\n","\n","      # CLASSIFIER\n","      indices = np.random.randint(0, X_train.shape[0], size=2*batchSize)\n","      imageBatch = X_train[indices,:]\n","      imageBatch_labels = Y_train[indices,:]\n","      imageBatch_perturbated = imageBatch_mix(batchSize)\n","      X_disc, y_disc, weights = data_train_classifier(imageBatch, imageBatch_labels, imageBatch_perturbated)#pour l'instant cette fonction ne prend pas en compte les données perturbées => à coder\n","      if (X_disc.shape[0]> 10):\n","        scores = classifier.train_on_batch(X_disc, y_disc)\n","        closs += scores[0]\n","\n","      # REJECTOR\n","      X_disc, y_disc, weights = data_train_rejector(imageBatch, imageBatch_labels, allgeneratedImages, imageBatch_perturbated)\n","      scores = rejector.train_on_batch(X_disc, y_disc)\n","      rloss += scores[0]\n","\n","    # TESTS ET EVALUATIONS\n","    d_losses.append(dloss)\n","    g_losses.append(gloss)\n","    r_losses.append(rloss)\n","    c_losses.append(closs)\n","\n","    X_disc, y_disc = data_evaluation_rejector_true(X_train)\n","    r_train_in = rejector.evaluate(X_disc, y_disc)\n","    X_disc, y_disc = data_evaluation_rejector_true(X_test_in)\n","    r_test_in = rejector.evaluate(X_disc, y_disc)\n","    X_disc, y_disc = data_evaluation_rejector_fake(X_test_out, filtre=False)\n","    r_test_out = rejector.evaluate(X_disc, y_disc)\n","    X_disc, y_disc = data_evaluation_rejector_fake(allgeneratedImages, filtre=True)\n","    r_generated_filtre = rejector.evaluate(X_disc, y_disc)\n","    X_disc, y_disc = data_evaluation_rejector_fake(allgeneratedImages, filtre=False)\n","    r_generated = rejector.evaluate(X_disc, y_disc)\n","    X_disc, y_disc = data_evaluation_classifier(X_test,Y_test)\n","    a_eval_pred = classifier.evaluate(X_disc, y_disc)\n","    a_pred_in = classifier.evaluate(X_test_in, Y_test_in)\n","    a_pred_out = classifier.evaluate(X_test_out, Y_test_out)\n","\n","    print (\"Nombre d'exemples générés : \", allgeneratedImages.shape[0])\n","    print (\"Score du REJECTOR sur les données d'entraînement de MNIST : \", r_train_in)\n","    print (\"Score de REJECTOR sur les données générées par le GAN : \", r_generated)\n","    print (\"Score du REJECTOR sur les données générées par le GAN filtrées : \", r_generated_filtre)\n","    print (\"Score du REJECTOR sur les données de test de MNIST des classes attendues : \", r_test_in)\n","    print (\"Score du REJECTOR sur les données de test de MNIST des classes non attendues : \", r_test_out)\n","    print (\"Score du CLASSIFIER sur les données de test de MNIST non rejetées : \", a_eval_pred)\n","    print (\"Score du CLASSIFIER sur les données de test de MNIST des classes attendues : \", a_pred_in)\n","    print (\"Score du CLASSIFIER sur les données de test de MNIST des classes non attendues : \", a_pred_out)\n","\n","    Xin,Yin = data_evaluation_rejector_true(X_test_in)\n","    y_pred_in = rejector.predict(Xin)\n","    Xout,Yout = data_evaluation_rejector_fake(X_test_out, filtre=False)\n","    y_pred_out = rejector.predict(Xout)\n","    Ytrue = np.concatenate ((Yout,Yin))\n","    Ypred = np.concatenate ((y_pred_out,y_pred_in))\n","\n","    print (\"Prédictions du REJECTOR sur les données de test de MNIST des classes attendues : Min, \", np.min(y_pred_in), \" Max\", np.max(y_pred_in), ' Moyenne : ', np.mean(y_pred_in))\n","    print (\"Prédictions du REJECTOR sur les données de test de MNIST des classes non attendues : Min, \", np.min(y_pred_out), \" Max\", np.max(y_pred_out), ' Moyenne : ', np.mean(y_pred_out))\n","    print(\"Performances du REJECTOR sur toutes les données de test de MNIST :\")\n","    print (\"F1 : \", f1(Ytrue,Ypred), \"Précision : \", precision(Ytrue,Ypred), \"Recall rate : \", recall(Ytrue,Ypred))\n","    print(\" Area Under Curve : \" ,  auc(Ytrue, Ypred))\n","    print(\"\")\n","    print (\"Losses : DISCRIMINATOR \", d_losses,\" GENERATOR \", g_losses,\" CLASSIFIER \", c_losses,\" REJECTOR \", r_losses)\n","    \n","    #SAUVEGARDER ET MONTRER LES IMAGES GENEREES REGULIEREMENT\n","    if e == 1 or e % 5 == 0:\n","      plotGeneratedImages(e)\n","      saveModels(e)\n","    \n","    #MODIFICATION DU ALPHA DES RBF LAYERS\n","    b = rejector.layers[8].get_weights()\n","    alpha = b[0][0]\n","    alpha = alpha * Decrease_rate_alpha        \n","    b[0][0]=alpha\n","    rejector.layers[8].set_weights(b)\n","  plotLoss(e)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fk3QqN5dftrB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"form"},"cell_type":"code","source":["#@title ENTRAINEMENT DU GAN SEUL (télécharge toutes les epochs)\n","def train_GAN(X_train, Y_train, epochs=100, batchSize=128):\n","  #INITIALISATION...\n","  batchCount = int(X_train.shape[0] / batchSize)\n","  print ('Epochs:', epochs)\n","  print ('Batch size:', batchSize)\n","  print ('Batches per epoch:', batchCount)\n","\n","  for e in range(1, epochs+1):\n","    print ('-'*15, 'Epoch %d' % e, '-'*15)\n","    dloss=0\n","    gloss=0\n","    for _ in tqdm(range(batchCount)):\n","      # DISCRIMINATOR\n","      noise = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      generatedImages = generator.predict(noise)\n","      imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n","      X = np.concatenate([imageBatch, generatedImages])\n","      yDis = np.zeros(2*batchSize)\n","      yDis[:batchSize] = 0.9\n","      dloss += discriminator.train_on_batch(X, yDis)\n","\n","      # GENERATOR\n","      noise = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      yGen = np.ones(batchSize)\n","      discriminator.trainable = False\n","      gloss += gan.train_on_batch(noise, yGen)\n","      \n","    # TESTS ET EVALUATIONS\n","    d_losses.append(dloss)\n","    g_losses.append(gloss)\n","    print (\"Losses : DISCRIMINATOR \", d_losses[-1],\" GENERATOR \", g_losses[-1])\n","    \n","    saveModels(e)\n","    downloadModels(e)\n","    plotLoss(e)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wmRrIrubgGxE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"form"},"cell_type":"code","source":["#@title ENTRAINEMENT DU REJECTOR SEUL (télécharge la dernière epoch) (WIP)\n","def train_rejector_on_bdd(X_train, bdd, batchSize=64):\n","  #INITIALISATION...\n","  if bdd == 1:\n","    genID = drive.ListFile({'title':'generator_e%d' % epoch}).GetList()[0]['id']\n","  batchCount = int(X_train.shape[0] / batchSize)\n","  print ('Epochs:', epochs)\n","  print ('Batch size:', batchSize)\n","  print ('Batches per epoch:', batchCount)\n","\n","  for e in range(1, epochs+1):\n","    print ('-'*15, 'Epoch %d' % e, '-'*15)\n","    rloss=0\n","    for _ in tqdm(range(batchCount)):\n","      # DISCRIMINATOR\n","      noise = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      generatedImages = generator.predict(noise)\n","      imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n","      X = np.concatenate([imageBatch, generatedImages])\n","      yDis = np.zeros(2*batchSize)\n","      yDis[:batchSize] = 0.9\n","      dloss += discriminator.train_on_batch(X, yDis)\n","\n","      # GENERATOR\n","      noise = np.random.normal(0, 1, size=[batchSize, randomVectorSize])\n","      yGen = np.ones(batchSize)\n","      discriminator.trainable = False\n","      gloss += gan.train_on_batch(noise, yGen)\n","      \n","    # TESTS ET EVALUATIONS\n","    d_losses.append(dloss)\n","    g_losses.append(gloss)\n","    print (\"Loss : REJECTOR \", r_losses[-1])\n","    \n","    saveModels(e)\n","    downloadModels(e)\n","    plotLoss(e)\n","    \n","    #MODIFICATION DU ALPHA DES RBF LAYERS\n","    b = rejector.layers[8].get_weights()\n","    alpha = b[0][0]\n","    alpha = alpha * Decrease_rate_alpha        \n","    b[0][0] = alpha\n","    rejector.layers[8].set_weights(b)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mmIgmB8U-PCA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"form"},"cell_type":"code","source":["#@title GENERATION DE LA BASE DE DONNEES (méthode 1.1/1.2)\n","def bdd_1_1(epochs, batchSize=64, batchNumber=50, qualityCheck='current', GANepochs=99, maxTry = 512, minTry = 64, shuffle = True):\n","  # QUALITYCHECK : 'current' ou 'final', passe de 1) à 2)\n","  bdd = np.zeros(epochs,batchSize*batchNumber,28,28,1)\n","  for i in range(epochs):\n","    length = 0\n","    final_length = batchSize*batchNumber\n","    epoch_bdd = np.zeros((final_length,28,28,1))\n","    generator, discriminator = uploadModels(i)\n","    if qualityCheck == 'final':\n","      discriminator = uploadModels(GANepochs)[1]\n","    while length != final_length:\n","      compute = max([min([final_length-length,maxTry]),minTry])\n","      noise = np.random.normal(0, 1, size=[compute, randomVectorSize])\n","      examples = generator.predict(noise)\n","      scores = discriminator.predict(examples)\n","      examples = examples[np.where(scores[:,0] < seuil_rejet_RejectionTraining)]\n","      for nb,i in enumerate(examples):\n","        if length < final_length:\n","          epoch_bdd[length,:] = i[:]\n","          length += 1\n","    if shuffle:\n","      np.random.shuffle(epoch_bdd)\n","    bdd[i,:] = epoch_bdd[:]\n","    with open(\"bdd_1_1_e\"+str(i),'wb') as file:\n","      m=pickle.Pickler(file)\n","      m=pickle.dump(epoch_bdd,file)\n","    uploaded = drive.CreateFile({'title': 'bdd_1_1_e%d' % i})\n","    uploaded.SetContentFile('bdd_1_1_e%d' % i)\n","    uploaded.Upload()\n","  return bdd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"47eQzBqGGIs4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"form"},"cell_type":"code","source":["#@title GENERATION DE LA BASE DE DONNEES (méthode 1.3/1.4)\n","def bdd_1_2(epochs, batchSize=64, batchNumber=50, qualityCheck='current', GANepochs=99, maxTry = 512, minTry = 64, shuffle = True):\n","  # QUALITYCHECK : 'current' ou 'final', passe de 1) à 2)\n","  bdd = np.zeros(epochs,batchSize*batchNumber,28,28,1)\n","  for i in range(epochs):\n","    length = 0\n","    if i == 0:\n","      final_length = batchSize*batchNumber\n","    else:\n","      final_length = batchSize*batchNumber/2\n","    epoch_bdd = np.zeros((batchSize*batchNumber,28,28,1))\n","    generator, discriminator = uploadModels(i)\n","    if qualityCheck == 'final':\n","      discriminator = uploadModels(GANepochs)[1]\n","\n","    #PARTIE 1 : EXEMPLES DE L'EPOCH CORRESPONDANTE\n","    while length != final_length:\n","      compute = max([min([final_length-length,maxTry]),minTry])\n","      noise = np.random.normal(0, 1, size=[compute, randomVectorSize])\n","      examples = generator.predict(noise)\n","      scores = discriminator.predict(examples)\n","      examples = examples[np.where(scores[:,0] < seuil_rejet_RejectionTraining)]\n","      for nb,i in enumerate(examples):\n","        if length < final_length:\n","          epoch_bdd[length,:] = i[:]\n","          length += 1\n","\n","    #PARTIE 2 : EXEMPLES DES EPOCHS PRECEDENTES\n","    numberPerEpoch = ceil((batchSize*batchNumber - final_length) / float(i))\n","    for j in range(i):\n","      generator, discriminator = uploadModels(j)\n","      if qualityCheck == 'final':\n","        discriminator = uploadModels(GANepochs)[1]\n","      while length <= final_length + numberPerEpoch*(j+1) and length != batchSize*batchNumber:\n","        compute = max([min([final_length-length,maxTry]),minTry])\n","        noise = np.random.normal(0, 1, size=[compute, randomVectorSize])\n","        examples = generator.predict(noise)\n","        scores = discriminator.predict(examples)\n","        examples = examples[np.where(scores[:,0] < seuil_rejet_RejectionTraining)]\n","        for nb,i in enumerate(examples):\n","          if length < batchSize*batchNumber:\n","            epoch_bdd[length,:] = i[:]\n","            length += 1\n","\n","    #PARTIE 3 : ENREGISTREMENT\n","    if shuffle:\n","      np.random.shuffle(epoch_bdd)\n","    bdd[i,:] = epoch_bdd[:]\n","    with open(\"bdd_1_2_e\"+str(i),'wb') as file:\n","      m=pickle.Pickler(file)\n","      m=pickle.dump(epoch_bdd,file)\n","    uploaded = drive.CreateFile({'title': 'bdd_1_2_e%d' % i})\n","    uploaded.SetContentFile('bdd_1_2_e%d' % i)\n","    uploaded.Upload()\n","  return bdd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zMwwBlWaLY4i","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"form"},"cell_type":"code","source":["#@title GENERATION DE LA BASE DE DONNEES (méthode 1.7)\n","def bdd_1_3(epochs, batchSize=64, batchNumber=50, qualityCheck='current', GANepochs=99, maxTry = 512, minTry = 64, shuffle = True):\n","  # QUALITYCHECK : 'current' ou 'final', passe de 1) à 2)\n","  bdd = np.zeros(epochs*batchSize*batchNumber,28,28,1)\n","  #repartition est telle que sum(repartition) = epochs*batchSize*batchNumber\n","  repartition = [epochs*batchSize*batchNumber/GANepochs for i in range(GANepochs)]\n","  while sum(repartition) != epochs*batchSize*batchNumber:\n","    repartition[len(repartition)//2]+=1\n","  length = 0\n","\n","  #PARTIE 1 : RASSEMBLEMENT DES EXEMPLES\n","  for i in range(epochs):\n","    final_length = length + repartition[i]\n","    generator, discriminator = uploadModels(i)\n","    if qualityCheck == 'final':\n","      discriminator = uploadModels(GANepochs)[1]\n","    while length != final_length and length != batchSize*batchNumber:\n","      compute = max([min([final_length-length,maxTry]),minTry])\n","      noise = np.random.normal(0, 1, size=[compute, randomVectorSize])\n","      examples = generator.predict(noise)\n","      scores = discriminator.predict(examples)\n","      examples = examples[np.where(scores[:,0] < seuil_rejet_RejectionTraining)]\n","      for nb,i in enumerate(examples):\n","        if length < final_length:\n","          bdd[length,:] = i[:]\n","          length += 1\n","  \n","  #PARTIE 2 : TRI ET ENREGISTREMENT\n","  discriminator = uploadModels(GANepochs)[1]\n","  scores = discriminator.predict(bdd)\n","  scores_sorted = [(i,scores[i]) for i in range(scores.shape[0])]\n","  scores_sorted.sort(key = lambda x:x[1])\n","  order = [i[0] for i in scores_sorted]\n","  bdd = bdd[order,:]\n","  bdd_to_return = np.zeros(epochs,batchSize*batchNumber,28,28,1)\n","  for i in range(epochs):\n","    to_shuffle = bdd[i*batchSize*batchNumber:(i+1)*batchSize*batchNumber,:]\n","    if shuffle:\n","      np.random.shuffle(to_shuffle)\n","    bdd_to_return[i,:] = to_shuffle[:]\n","    with open(\"bdd_1_3_e\"+str(i),'wb') as file:\n","      m=pickle.Pickler(file)\n","      m=pickle.dump(to_shuffle,file)\n","    uploaded = drive.CreateFile({'title': 'bdd_1_3_e%d' % i})\n","    uploaded.SetContentFile('bdd_1_3_e%d' % i)\n","    uploaded.Upload()\n","  return bdd_to_return"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D7vtX29sY5r-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"form"},"cell_type":"code","source":["#@title GENERATION DE LA BASE DE DONNEES (méthode 2 avec répartition imposée) (WIP)\n","def bdd_2_1(epochs, repartition = None, batchSize=64, batchNumber=50, qualityCheck='current', GANepochs=99, maxTry = 512, minTry = 64, shuffle = True):\n","  bdd = np.zeros(epochs*batchSize*batchNumber,28,28,1)\n","  #repartition est telle que sum(repartition) = epochs*batchSize*batchNumber\n","  if repartition == None:\n","    repartition = [epochs*batchSize*batchNumber/GANepochs for i in range(GANepochs)]\n","    while sum(repartition) != epochs*batchSize*batchNumber:\n","      repartition[len(repartition)//2]+=1\n","  length = 0\n","\n","  #PARTIE 1 : RASSEMBLEMENT DES EXEMPLES\n","  for i in range(epochs):\n","    final_length = length + repartition[i]\n","    generator, discriminator = uploadModels(i)\n","    if qualityCheck == 'final':\n","      discriminator = uploadModels(GANepochs)[1]\n","    while length != final_length and length != batchSize*batchNumber:\n","      compute = max([min([final_length-length,maxTry]),minTry])\n","      noise = np.random.normal(0, 1, size=[compute, randomVectorSize])\n","      examples = generator.predict(noise)\n","      scores = discriminator.predict(examples)\n","      examples = examples[np.where(scores[:,0] < seuil_rejet_RejectionTraining)]\n","      for nb,i in enumerate(examples):\n","        if length < final_length:\n","          bdd[length,:] = i[:]\n","          length += 1\n","  \n","  #PARTIE 2 : TRI ET ENREGISTREMENT\n","  discriminator = uploadModels(GANepochs)[1]\n","  scores = discriminator.predict(bdd)\n","  scores_sorted = [(i,scores[i]) for i in range(scores.shape[0])]\n","  scores_sorted.sort(key = lambda x:x[1])\n","  order = [i[0] for i in scores_sorted]\n","  bdd = bdd[order,:]\n","  bdd_to_return = np.zeros(epochs,batchSize*batchNumber,28,28,1)\n","  for i in range(epochs):\n","    to_shuffle = bdd[i*batchSize*batchNumber:(i+1)*batchSize*batchNumber,:]\n","    if shuffle:\n","      np.random.shuffle(to_shuffle)\n","    bdd_to_return[i,:] = to_shuffle[:]\n","    with open(\"bdd_2_e\"+str(i),'wb') as file:\n","      m=pickle.Pickler(file)\n","      m=pickle.dump(to_shuffle,file)\n","    uploaded = drive.CreateFile({'title': 'bdd_2_e%d' % i})\n","    uploaded.SetContentFile('bdd_2_e%d' % i)\n","    uploaded.Upload()\n","  return bdd_to_return, weights"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DVPoew9jz6fT","colab_type":"text"},"cell_type":"markdown","source":["# Lancements"]},{"metadata":{"id":"ByQwwzQDROE-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":3403},"cellView":"both","outputId":"4cc92842-04dd-44fc-f444-ca45f41cee30"},"cell_type":"code","source":["train_GAN(X_train, Y_train, epochs=100, batchSize=128)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/468 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epochs: 100\n","Batch size: 128\n","Batches per epoch: 468\n","--------------- Epoch 1 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","100%|██████████| 468/468 [00:43<00:00, 10.80it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  306.69124668836594  GENERATOR  388.0207463502884\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:55,  8.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 2 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.59it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  293.7044338583946  GENERATOR  427.17947697639465\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:39, 11.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 3 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.59it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  278.7385693192482  GENERATOR  460.9606719613075\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:52,  8.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 4 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.48it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  274.2025620341301  GENERATOR  474.27727603912354\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 5 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.51it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  272.55792969465256  GENERATOR  484.6144567131996\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:57,  8.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 6 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.64it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  273.6956526041031  GENERATOR  490.37773448228836\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:43, 10.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 7 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  277.81827265024185  GENERATOR  487.555355489254\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:52,  8.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 8 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.47it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  287.3891124725342  GENERATOR  469.0167478919029\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:41, 11.10it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 9 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.51it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  294.73087871074677  GENERATOR  454.357248544693\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:43, 10.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 10 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  298.2708948254585  GENERATOR  442.387757062912\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 11 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.63it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  301.78410279750824  GENERATOR  435.3345704674721\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:54,  8.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 12 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.49it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  302.4444164633751  GENERATOR  429.81664168834686\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:43, 10.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 13 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.51it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  301.7761541008949  GENERATOR  430.6789246201515\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:53,  8.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 14 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.60it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.47428584098816  GENERATOR  430.52369153499603\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 15 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.59it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.2625772356987  GENERATOR  428.3818881511688\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:52,  8.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 16 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.49it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.9137204885483  GENERATOR  430.66848200559616\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 17 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.50it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  301.01807576417923  GENERATOR  429.66234934329987\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:57,  8.07it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 18 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.56it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.6014212965965  GENERATOR  429.1611284017563\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:41, 11.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 19 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.72650200128555  GENERATOR  428.8334830403328\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:54,  8.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 20 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.47it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.29039776325226  GENERATOR  429.38508558273315\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:41, 11.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 21 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.51it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  299.5438475012779  GENERATOR  428.200581908226\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py:528: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n","  max_open_warning, RuntimeWarning)\n","  0%|          | 1/468 [00:00<00:54,  8.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 22 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.58it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  299.9428464770317  GENERATOR  428.73484683036804\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:42, 10.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 23 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.65it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  299.947576880455  GENERATOR  429.7374601364136\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:57,  8.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 24 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.53it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  299.87608730793  GENERATOR  428.3948159813881\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 25 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.55it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.1265849471092  GENERATOR  428.68521851301193\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:55,  8.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 26 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.47it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.04168993234634  GENERATOR  428.16756850481033\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 27 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.3212223649025  GENERATOR  427.2034685611725\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:56,  8.26it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 28 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.52it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.7616294026375  GENERATOR  427.86434680223465\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 29 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.54it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.2134671807289  GENERATOR  427.99980545043945\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:55,  8.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 30 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.53it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.3607060313225  GENERATOR  426.4417744278908\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:39, 11.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 31 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.69it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.46439546346664  GENERATOR  427.1121680736542\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:54,  8.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 32 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.58it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.7331175208092  GENERATOR  427.6175279021263\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 33 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.54it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.3925651907921  GENERATOR  426.2931718826294\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/468 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 34 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.49it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.23958146572113  GENERATOR  427.6825929284096\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:42, 10.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 35 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.65it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.5189773440361  GENERATOR  427.6061207652092\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:55,  8.44it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 36 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.61it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.33666026592255  GENERATOR  428.00111120939255\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:41, 11.36it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 37 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.52it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.5134397149086  GENERATOR  427.88083428144455\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:53,  8.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 38 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.48it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.24696439504623  GENERATOR  428.38525503873825\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:50,  9.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 39 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.56it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.3820362687111  GENERATOR  428.0718423128128\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:41, 11.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 40 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.63it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.4188276529312  GENERATOR  427.570718228817\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 41 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.54it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.09445917606354  GENERATOR  428.9391429424286\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:51,  9.02it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 42 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.49it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  299.7222578525543  GENERATOR  428.88750898838043\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:43, 10.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 43 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.59it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  300.26389157772064  GENERATOR  428.77037900686264\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:51,  9.02it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 44 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  299.91617250442505  GENERATOR  429.6568692922592\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:39, 11.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 45 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  299.8158137202263  GENERATOR  427.72752141952515\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:56,  8.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 46 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.51it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  299.66885286569595  GENERATOR  429.2715634703636\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 47 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.63it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  299.0783650279045  GENERATOR  429.0032588839531\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:55,  8.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 48 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  299.19677472114563  GENERATOR  430.5496461391449\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:41, 11.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 49 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.65it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  299.1402820944786  GENERATOR  431.1987706422806\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:49,  9.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 50 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.60it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  298.6457197070122  GENERATOR  430.8279629945755\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 51 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  298.82298135757446  GENERATOR  430.7474176287651\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:54,  8.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 52 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:39<00:00, 11.72it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  298.1537534594536  GENERATOR  432.08334642648697\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:41, 11.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 53 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.65it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  298.8408743739128  GENERATOR  432.58030730485916\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:50,  9.19it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 54 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.56it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  298.5148105621338  GENERATOR  432.2327598929405\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:39, 11.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 55 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  297.92537492513657  GENERATOR  432.1141711473465\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:56,  8.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 56 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.69it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  298.19266575574875  GENERATOR  432.84599101543427\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:39, 11.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 57 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  297.81598448753357  GENERATOR  433.7550509572029\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:56,  8.26it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 58 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.54it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  297.8959009051323  GENERATOR  433.4999811053276\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:40, 11.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 59 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.58it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  297.7800814509392  GENERATOR  435.35628551244736\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:53,  8.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 60 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.65it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  297.8013120293617  GENERATOR  435.2595207095146\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:38, 11.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 61 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  297.32868897914886  GENERATOR  434.4750814437866\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:54,  8.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 62 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.58it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  297.3187221288681  GENERATOR  435.59476095438004\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2/468 [00:00<00:39, 11.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 63 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 468/468 [00:40<00:00, 11.59it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Losses : DISCRIMINATOR  297.0547565817833  GENERATOR  435.8218546509743\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/468 [00:00<00:55,  8.44it/s]"],"name":"stderr"},{"output_type":"stream","text":["--------------- Epoch 64 ---------------\n"],"name":"stdout"},{"output_type":"stream","text":[" 32%|███▏      | 148/468 [00:12<00:27, 11.48it/s]"],"name":"stderr"}]},{"metadata":{"id":"48rcbcOHN0b-","colab_type":"text"},"cell_type":"markdown","source":["# TESTS"]},{"metadata":{"id":"RBbBWo8EN1EW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#@title SEUILLAGES\n","\n","\"\"\"Importer le modèle à 95-96%\"\"\"\n","\n","model_95=load_model('10_classifier')\n","\n","#entropie\n","seuils=np.concatenate((np.linspace(3.3,3.32,4),np.linspace(3.32,3.323,20)))\n","x=[]\n","y=[]\n","lim=10001\n","current=0\n","i=0\n","while i<len(seuils) and current<lim:\n","  print(str(i+1)+\"/\"+str(len(seuils)))\n","  point=test_model_entropie(model_95,seuils[i],x_test,y_test)\n","  x.append(point[0])\n","  y.append(point[1])\n","  i+=1\n","  current=point[0]\n","\n","  \n","o=np.ones(len(x))\n","plt.plot(x,y,'b',label=\"entropie\")\n","plt.plot(x,o,'r')\n","\n","#max\n","seuils=np.concatenate((np.linspace(0,0.1,6),np.linspace(0.1,0.11,10),np.linspace(0.11,1,2)))\n","x=[]\n","y=[]\n","lim=10000\n","current=0\n","i=0\n","while i<len(seuils) and current<lim:\n","  print(str(i+1)+\"/\"+str(len(seuils)))\n","  point=test_model(model_95,seuils[i],x_test,y_test)\n","  x.append(point[0])\n","  y.append(point[1])\n","  print(point)\n","  i+=1\n","  current=point[0]\n","\n","\n","plt.plot(x,y,'g',label=\"max\")\n","plt.xlabel(\"Nombre d'images rejetées\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k61aLiuIP8BJ","colab_type":"text"},"cell_type":"markdown","source":["# DERNIERS CHANGEMENTS / ETAT DES LIEUX"]},{"metadata":{"id":"hdItkPN9QCoK","colab_type":"text"},"cell_type":"markdown","source":["Ajout du code d'Artieres : TERMINE\n","\n","Test du code d'Artieres : OK\n","- Entraînement : OK\n","- Tests : OK\n","- Autres fonctions : OK\n","\n","Codes d'entraînement séparés : EN COURS\n","- GAN seul : A TESTER\n","- Création BDD : A FAIRE\n","- REJECTOR seul : A FAIRE\n","- CLASSIFIEUR seul : A ECRIRE (importer le modèle déjà prêt)\n","\n","Nouvelles fonctions de choix des exemples : A VENIR\n","\n","METHODE 1 : Curriculum learning\n","\n","Plusieurs possibilités d'apprentissage progressif :\n","\n","1) Chaque epoch d'apprentissage du REJECTOR est composée d'exemples générés par l'epoch correspondante du generator et filtrés par le DISCRIMINATOR correspondant\n","\n","2) Même chose mais on filtre avec le DISCRIMINATOR final\n","\n","3) Chaque epoch d'apprentissage du REJECTOR est composée d'exemples générés par l'epoch correspondante et d'exemples générés précédemment, filtrés par le DISCRIMINATOR correspondant\n","\n","4) Même chose mais on filtre avec le DISCRIMINATOR final\n","\n","[5) Pour chaque epoch, le nième batch est constitué d'images générées par la nième epoch du GAN, filtrées par le DISCRIMINATOR correspondant]\n","\n","[6) Même chose mais on filtre avec le DISCRIMINATOR final]\n","\n","7) On commence par générer une grande base de données avec toutes les epochs du GAN, que l'on ordonne grâce à leurs score par le DISCRIMINATOR final pour créer les batchs des différentes epochs\n","\n","Priorité : METHODE 2 : Traditionnal learning\n","\n","On commence par générer une base de données avec les différentes epochs du GAN, pour lesquelles on a les options suivantes :\n"," - filtrer par le DISCRIMINATOR correspondant ou le DISCRIMINATOR final\n"," - attribuer un poids grâce au score du DISCRIMINATOR final ou non\n"," - inclure des exemples de MNIST découpés ou non, que l'on peut tester avec le DISCRIMINATOR final ou non\n"," - mélanger la base de données avant de commencer ou non\n"," - imposer la répartition des exemples entre les différentes epochs du GAN ou laisser générer en fonction du pourcentage d'exemples pertinents à chaque epochs\n","\n","\n","Pour la base de données positive, on choisit évidemment les classes retenues de MNIST, et on choisit les éléments de chaque batch aléatoirement avec remise.\n","\n","A AJOUTER :\n"," - utilisation de plusieurs GANs\n"," - autres méthodes génératives\n"," - changer les paramètres ou initialisations des mêmes GANs"]}]}